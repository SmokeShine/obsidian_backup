202311081514
Status: #idea
Tags: [[Explore Exploit]]

# Multi Armed Bandits

1. State space exploration
2. one arm explore - other arm exploit 
3. gap = action - optimal action(Stationary distribution)
	1. Regret = opportunity loss = sum(gaps)
4. We can explore but if it is incorrect, there will be a gap and regret will not come down
5. 
---
# References

1. https://www.youtube.com/watch?v=sGuiWX07sKw