202501211120
Status: #idea
Tags: [[Long Short-Term Memory (LSTM)]]

# Input Gate

1. Has two switches
	1. sigmoid - for learning harder patterns by compressing hidden state between 0 and 1
	2. tanh - for learning easier patterns, by compressing hidden state between -1 and 1
---
# References

1. https://ai.stackexchange.com/questions/32505/why-is-there-tanhxsigmoidx-in-a-lstm-cell