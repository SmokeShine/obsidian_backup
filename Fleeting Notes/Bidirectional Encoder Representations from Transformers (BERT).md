202411191159
Status: #idea
Tags:

# Bidirectional Encoder Representations from Transformers (BERT)

1. We can look at everything using MLP. For attention, we need to look at small parts. THis is called attention. Then we need to look from different point of view. This is multi headed. Then we need to combine this attention. This is stacking. 
2. BERT is masked modelling for encoding. There are different task to this backbone -  masked modelling, sentiment analysis, question answering, named entity recognition. Hugging face has pre trained models for them using benchmark data.
---
# References

1. https://learn.nvidia.com/courses/course?course_id=course-v1:DLI+C-FX-09+V2&unit=block-v1:DLI+C-FX-09+V2+type@vertical+block@e2b8cfd88f2a45c89ef908f7929c266c