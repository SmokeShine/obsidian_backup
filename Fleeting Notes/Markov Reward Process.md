202310241103
Status: #idea
Tags: [[MDP]]

# Markov Reward Process

1. [[Markov Chain]] + [[Reward]] function showing immediate scalar returned for being in a state + Discounting Factor
---
# References

1. https://www.youtube.com/watch?v=lfHX2hHRMVQ