202311091501
Status: #idea
Tags: [[Multi Armed Bandits]]

# Probability Matching

Select action according to probability that action is the optimal

From 3 distributions for taking actions, select one sample from each distribution using posterior (not prior). Use the action with the sample with maximum value

1) thompson sampling works for multi armed bandit and not for full MDP where we have sequence of actions to take

---
# References

1. https://www.youtube.com/watch?v=sGuiWX07sKw